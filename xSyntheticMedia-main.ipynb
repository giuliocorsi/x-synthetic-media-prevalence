{"cells":[{"cell_type":"markdown","metadata":{"id":"AIVmAXwA_1kz"},"source":["# Evaluating the Spread of AI-Generated Synthetic Media on X - Main\n","\n"]},{"cell_type":"markdown","metadata":{"id":"in0v7TiL_6R9"},"source":["## 1. Prepare Environment and Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGyXyb2xqtc3"},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","from datetime import datetime\n","drive.mount('/content/drive')\n","\n","DATA_LOC = \"path-to-notes-tsv-file\"\n","communitynotes_data = pd.read_csv(DATA_LOC, delimiter = '\\t')\n","\n","communitynotes_data[\"date\"] = pd.to_datetime(communitynotes_data['createdAtMillis'], unit='ms')\n","communitynotes_data[\"date\"] = communitynotes_data[\"date\"].dt.strftime('%d/%m/%Y')\n","communitynotes_data = communitynotes_data[(pd.to_datetime(communitynotes_data['date'], format='%d/%m/%Y') > '01-11-2022') & (pd.to_datetime(communitynotes_data['date'], format='%d/%m/%Y') < '30-09-2023')]"]},{"cell_type":"markdown","metadata":{"id":"9mi9uUZE_-J3"},"source":["## 2. Extract Community Notes by Keyword and Save Data as Dictionary"]},{"cell_type":"markdown","metadata":{"id":"uOEVKryvPcCG"},"source":["This query filters for instances of AI-generated visual content that could be misleading, such as \"deepfakes\" or AI-generated images. However, it deliberately excludes strings related to explicitly AI-generated content, such as the hashtag #aiart, focusing on capturing only those cases where the audience could potentially be misled due to the absence of clear labeling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LElJglrQOrw"},"outputs":[],"source":["import pandas as pd\n","import pickle\n","\n","def filter_dataframe(full_df, expressions):\n","    pattern = '|'.join(expressions)\n","    new_df = full_df[full_df['summary'].str.contains(pattern, case=False, na=False)]\n","    return new_df\n","\n","expressions = [\"ai-generated(?:\\s+image|\\s+video|\\s+art|\\s+photo|\\s+deepfake)\",\n","               \"ai generated(?:\\s+image|\\s+video|\\s+art|\\s+photo|\\s+deepfake)\",\n","               \"ai(?:\\s+image|\\s+video|\\s+art|\\s+photo|\\s+deepfake)\",\n","               \"ai-(?:\\s+image|\\s+video|\\s+art|\\s+photo|\\s+deepfake)\",\n","               \"generated\\s+(?:with|by)\\s+(?:ai|artificial intelligence)\",\n","               \"midjourney\", \"stable diffusion\", \"dall-e\", \"deepfake\",\n","               \"deep fake\", \"deepfaked\"]\n","\n","visual_disinformation = filter_dataframe(communitynotes_data, expressions)\n","no_visual_disinformation = communitynotes_data.drop(visual_disinformation.index)\n","\n","community_notes = {'visual': visual_disinformation, 'no_visual': no_visual_disinformation}\n","\n","with open('community-notes-filtered.pkl', 'wb') as f:\n","    pickle.dump(community_notes, f)"]},{"cell_type":"markdown","metadata":{"id":"U0u33E7sRF6-"},"source":["## 3. Collect Data From Source Tweets"]},{"cell_type":"markdown","metadata":{"id":"VjZRlbrojhyR"},"source":["Here leverage Selenium's headless browser capabilities to systematically obtain data from the tweets to which the filtered community notes refer. The scraped data encompasses a wide range of metrics, including usernames, follower counts, tweet impressions, retweets, likes, bookmarks, as well as the tweet content itself.  Additionally, this pipeline is designed to automatically extract and store images embedded within these tweets, saving them to a Google Drive folder for subsequent analysis. It should be noted that, due to Twitter's dynamic content loading, the current implementation is unable to capture video content."]},{"cell_type":"markdown","metadata":{"id":"15ImLbF5m_ex"},"source":["### 3.1 Prepare Environment and Create Chrome Driver for Selenium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9G-MWOimUfp"},"outputs":[],"source":["!pip install selenium\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","import time\n","import os\n","import re\n","import pandas as pd\n","import urllib.request\n","from google.colab import drive\n","\n","def create_chrome_driver():\n","    chrome_options = webdriver.ChromeOptions()\n","    chrome_options.add_argument('--headless')\n","    chrome_options.add_argument('--no-sandbox')\n","    chrome_options.add_argument('--disable-dev-shm-usage')\n","    return webdriver.Chrome(options=chrome_options)"]},{"cell_type":"markdown","metadata":{"id":"SAzaaDHvnDxE"},"source":["### 3.2 Run the Main Data Collection Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8oBB9s8f1XC"},"outputs":[],"source":["def generate_tweet_url(tweet_id):\n","    return f\"https://twitter.com/anyuser/status/{tweet_id}\"\n","\n","def collect_tweet_data(source_tweets,OUTPUT_DIR_IMAGES, OUTPUT_FILE_INTERMEDIATE):\n","    drive.mount('/content/drive')\n","    driver = create_chrome_driver()\n","    source_tweets['tweetUrl'] = source_tweets['tweetId'].apply(generate_tweet_url)\n","\n","    patterns = {\n","        'views': re.compile(r'([\\d,.]+[KkMm]?)\\s*\\n\\s*Views'),\n","        'reposts': re.compile(r'([\\d,.]+[KkMm]?)\\s*\\n\\s*Reposts'),\n","        'quotes': re.compile(r'([\\d,.]+[KkMm]?)\\s*\\n\\s*Quotes'),\n","        'likes': re.compile(r'([\\d,.]+[KkMm]?)\\s*\\n\\s*Likes'),\n","        'bookmarks': re.compile(r'([\\d,.]+[KkMm]?)\\s*\\n\\s*Bookmarks')\n","    }\n","\n","    df = pd.DataFrame(columns=['tweetId', 'tweetDate', 'username', 'count', 'tweetUrl', 'imageUrl', 'views', 'reposts', 'quotes', 'likes', 'bookmarks', 'text'])\n","    os.makedirs(OUTPUT_DIR_IMAGES, exist_ok=True)\n","\n","    for i, row in source_tweets.iterrows():\n","        tweet_id = row['tweetId']\n","        tweet_url = row['tweetUrl']\n","        print(f\"Processing tweet ID: {tweet_id}\")\n","\n","        driver.get(tweet_url)\n","        time.sleep(15)\n","\n","        new_row = {'tweetId': tweet_id, 'count': row['count'], 'tweetUrl': tweet_url}\n","\n","        try:\n","            new_row['tweetDate'] = driver.find_element(By.XPATH, '//time').text\n","            new_row['username'] = driver.find_elements(By.CSS_SELECTOR, 'div a')[6].text\n","            full_text = driver.find_elements(By.CSS_SELECTOR, 'div div')[0].text\n","            new_row['text'] = driver.find_elements(By.CSS_SELECTOR, 'div span')[18].text\n","            images = driver.find_elements(By.XPATH, '//img[@alt=\"Image\"]')\n","            if images:\n","                new_row['imageUrl'] = images[0].get_attribute('src')\n","                urllib.request.urlretrieve(new_row['imageUrl'], f\"{OUTPUT_DIR_IMAGES}/image_{tweet_id}.jpg\")\n","            else:\n","                new_row['imageUrl'] = None\n","\n","            for key, pattern in patterns.items():\n","                match = pattern.search(full_text)\n","                new_row[key] = match.group(1) if match else None\n","\n","        except Exception as e:\n","          print(f\"An error occurred: {e}\")\n","          for key in ['tweetDate', 'username', 'text', 'imageUrl', 'views', 'reposts', 'quotes', 'likes', 'bookmarks']:\n","             new_row[key] = \"TWEET-NOT-FOUND\"\n","\n","        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n","        df.to_csv(OUTPUT_FILE_INTERMEDIATE, index=False)\n","\n","        if (i + 1) % 40 == 0:\n","            print(\"Pausing for 10 minutes after processing 40 tweets.\")\n","            time.sleep(600)\n","\n","    driver.quit()\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbgjhe7e8kZ_"},"outputs":[],"source":["OUTPUT_DIR_IMAGES = \"path-to-images-folder\"\n","OUTPUT_FILE_INTERMEDIATE = \"path-to-intermediate-file.csv\"\n","\n","source_tweets = community_notes['visual']['tweetId'].value_counts().reset_index()\n","source_tweets.columns = ['tweetId', 'count']\n","\n","hydrated_df = collect_tweet_data(source_tweets,OUTPUT_DIR_IMAGES,OUTPUT_FILE_INTERMEDIATE)"]},{"cell_type":"markdown","metadata":{"id":"yOUtMDzXz2xY"},"source":["### 3.3. Collect Data on Follower Counts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"V3eHAFc1lrnx"},"outputs":[],"source":["def collect_followers_count(df,OUTPUT_FILE_USERS):\n","    driver = create_chrome_driver()\n","    df['userFollowers'] = None  # Initialize the userFollowers column\n","\n","    for i, row in df.iterrows():\n","        username = row.get('username', None)\n","\n","        if username is None or username == 'TWEET-NOT-FOUND':\n","            print(\"Skipping row with missing or TWEET-NOT-FOUND username.\")\n","            df.at[i, 'userFollowers'] = 'TWEET-NOT-FOUND'\n","            continue\n","\n","        username = username.lstrip('@')\n","        url = f\"https://twitter.com/{username}\"\n","        print(f\"Collecting followers count for: {username}\")\n","\n","        driver.get(url)\n","        time.sleep(15)\n","\n","        try:\n","            all_span_elements = driver.find_elements(By.CSS_SELECTOR, 'span span')\n","            for j, element in enumerate(all_span_elements):\n","                if element.text == 'Followers':\n","                    follower_count = all_span_elements[j - 1].text\n","                    df.at[i, 'userFollowers'] = follower_count\n","                    print(f\"Follower count for {username}: {follower_count}\")\n","                    break\n","\n","        except Exception as e:\n","            print(f\"An error occurred while collecting followers count for {username}: {e}\")\n","\n","        if (i + 1) % 40 == 0:\n","            print(\"Saving intermediate data.\")\n","            df.to_csv(OUTPUT_FILE_USERS, index=False)\n","            print(\"Pausing for 10 minutes.\")\n","            time.sleep(600)\n","\n","    driver.quit()\n","    df.to_csv(OUTPUT_FILE_USERS, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4c9id4u-XVv"},"outputs":[],"source":["OUTPUT_FILE_USERS = \"path-to-users-file.csv\"\n","\n","hydrated_df_users = collect_followers_count(hydrated_df)"]},{"cell_type":"markdown","metadata":{"id":"7418jgkXYEtn"},"source":["### 3.4 Data Cleaning"]},{"cell_type":"markdown","metadata":{"id":"yFgGL0Y3kres"},"source":["Finally, to ensure the data's reliability and consistency for subsequent analyses, we clean the data prior to its use. This involves the standardization of numerical conventions and the transformation of the dates into a standard format (day/month/year)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UModBA_NekVN"},"outputs":[],"source":["from datetime import datetime\n","\n","def clean_dataframe(df):\n","    current_year = \"2023\"\n","\n","    def transform_value(x):\n","        if pd.isna(x) or x == '': return '0'\n","        try:\n","            num = float(x.replace('M', '').replace('K', '').replace(',', ''))\n","            num *= 1e6 if 'M' in x else 1e3 if 'K' in x else 1\n","            return (f\"{num/1e6:.1f}m\" if num >= 1e6 else f\"{num/1e3:.1f}k\").rstrip('0').rstrip('.') if num >= 1e3 else str(int(num))\n","        except: return x\n","\n","    def transform_date(d):\n","        if not isinstance(d, str): return None\n","        try: return datetime.strptime(d, '%I:%M %p Â· %b %d, %Y').strftime('%d/%m/%Y')\n","        except: pass\n","        try: return datetime.strptime(d, '%b %d, %Y').strftime('%d/%m/%Y')\n","        except: pass\n","        try: return datetime.strptime(f\"{d}, {current_year}\", '%b %d, %Y').strftime('%d/%m/%Y')\n","        except: return None\n","\n","    for col in ['views', 'reposts', 'quotes', 'likes', 'bookmarks', 'userFollowers']:\n","        df[col] = df[col].astype(str).apply(transform_value)\n","\n","    df['tweetDate'] = df['tweetDate'].apply(transform_date)\n","    df.rename(columns={'count': 'notesCount'}, inplace=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-b4CeKIGek2U"},"outputs":[],"source":["OUTPUT_FILE_FINAL = \"path-to-clean-file.csv\"\n","processed_df = clean_dataframe(hydrated_df)\n","processed_df.to_csv(OUTPUT_FILE_FINAL, index=False)"]},{"cell_type":"markdown","metadata":{"id":"9Dt5sofLKnOY"},"source":["# Codebook For Data Annotation"]},{"cell_type":"markdown","metadata":{"id":"sO4x4uPqQXr_"},"source":["# **1. Introduction**\n","\n","This codebook provides guidance on the manual coding process applies to the X dataframe obtained by searching tweets obtained from the Community Notes data releases. The dataset contains 16 columns, each capturing specific numeric and categorial aspects of a tweets.\n","\n","| Column Name  | Description                                                 |\n","|--------------|-------------------------------------------------------------|\n","| tweetId      | Unique identifier for each tweet                            |\n","| tweetDate    | Date when the tweet was posted                   |\n","| username     | Username of the account that posted the tweet               |\n","| userFollowers| Number of followers of the user who posted the tweet        |\n","| verified     | Indicator if the user account is verified (TRUE/FALSE)      |\n","| noteCount    | Total count of community notes obtained by the tweet.       |\n","| tweetUrl     | URL directing to the tweet                                  |\n","| imageUrl     | URL of the accompanying image of the tweet                  |\n","| views        | Number of times the tweet has been viewed                   |\n","| reposts      | Number of times the tweet has been reposted                 |\n","| quotes       | Number of times the tweet has been quoted                   |\n","| likes        | Number of likes the tweet has received                      |\n","| bookmarks    | Number of times the tweet has been bookmarked               |\n","| political    | Label indicating if the tweet is POLITICAL or NON-POLITICAL |\n","| media        | Type of media attached to the tweet (IMAGE or VIDEO)        |\n","| text         | The text content of the tweet                               |\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p5rTHrXEVBj1"},"source":["--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"QASF88LuSDBq"},"source":["# **2. Variables and Coding Instructions**\n"]},{"cell_type":"markdown","metadata":{"id":"8NbhueDgR7YH"},"source":["### **Status of Tweet**\n","\n","**REMOVED**: If a tweet is no longer existing or accessible during the inspection, it is marked as 'REMOVED'.\n","\n","**NO-DATA**: If a tweet was posted before the rollout of impression data in  and hence lacks impressions data, it is labeled as 'NO-DATA'. The data cutoff of impressions data can be quite inconsistent as this feature was rolled out progressively between November and December 2022\n","\n","**NOT-AI-GEN**: If the media accompanying the tweet is not AI-generated, as determined by assessing community notes consensus, label the entry as 'NOT-AI-GEN'."]},{"cell_type":"markdown","metadata":{"id":"MtiRg06JSuET"},"source":["--------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"e9UiBZdmSIDA"},"source":["### **User's Verified Status (Column Name: Verified)**\n","\n","**TRUE**: The user posting the tweet has a verification tick. This includes any color of verification tick and also encompasses premium tiers such as organization verification.\n","\n","**FALSE**: The user does not have a verification tick.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fZXQ9n_QSxur"},"source":["--------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"RvLWAt1WSj3I"},"source":["### **Political Status (Column Name: Political)**\n","\n","**POLITICAL**: Tweets that explicitly or implicitly referred to politicians, governments, political parties, or political issues\n","\n","\n","**NON-POLITICAL**: Tweets that do not meet the above criteria are labeled as 'NON-POLITICAL'."]},{"cell_type":"markdown","metadata":{"id":"jApj81sQSy_-"},"source":["--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"vnemWR1BTTRC"},"source":["# **3. Labelling Workflow**\n"]},{"cell_type":"markdown","metadata":{"id":"MTIhSOh7Nfrj"},"source":["**Initial Inspection**: Review each row to assess the status of the tweet. If the tweet has been removed or does not exist, label the status column as 'REMOVED'. If impression data is missing, label it as 'NO-DATA'.\n","AI Image Detection: Inspect the image accompanying each tweet to determine if it is AI-generated. If it is not, mark the status column as 'NOT-AI-GEN'. This last feature annotation will be based on community notes consensus noted during the inspection process.\n","\n","**Verification Check**: Examine the profile of the user who posted the tweet and determine if they have a verification tick. Update the verified column accordingly with 'TRUE' or 'FALSE'.\n","\n","**Political Nature**: Assess the content of the tweet to determine if it portrays a political figure. Update the POLITICAL column with either 'POLITICAL' or 'NON-POLITICAL' based on the evaluation."]},{"cell_type":"markdown","metadata":{"id":"hUg9yUAkVGJm"},"source":["--------------------------------------------------------------------------------\n","--------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"_v7wJj-gT0EC"},"source":["# **4. Krippendorph's Alpha**\n"]},{"cell_type":"markdown","metadata":{"id":"gRzjMkLRV6zZ"},"source":["To ensure the consistency and reliability of the manual coding process, we employ **Krippendorff's Alpha** as a statistical measure of agreement among multiple raters. This  method allows us to evaluate the reliability across different levels of measurement, making it particularly suitable for a dataset with diverse variables like ours. Computing Krippendorff's Alpha helps validate the manual annotations, thereby adding additional rigor to our annotation process. <Add this later>"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
